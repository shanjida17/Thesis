{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tflearn\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import json\n",
    "import pickle\n",
    "import nltk\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Swarna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Swarna\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Swarna\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt') #using punkt tokenizer\n",
    "nltk.download('wordnet') #using the wordnet dictonary\n",
    "nltk.download('omw-1.4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'intents': [{'tag': 'greeting', 'patterns': ['Hi', 'Hey', 'Is anyone there?', 'Hello', 'Hay'], 'responses': ['Hello', 'Hi', 'Hi there'], 'context': ['']}, {'tag': 'goodbye', 'patterns': ['Bye', 'See you later', 'Goodbye'], 'responses': ['See you later', 'Have a nice day', 'Bye! Come back again'], 'context': ['']}, {'tag': 'thanks', 'patterns': ['Thanks', 'Thank you', \"That's helpful\", 'Thanks for the help'], 'responses': ['Happy to help!', 'Any time!', 'My pleasure', \"You're most welcome!\"], 'context': ['']}, {'tag': 'about', 'patterns': ['Who are you?', 'What are you?', 'Who you are?'], 'responses': [\"I'm a bot assistant\", \"I'm Joana, a master's university recommendation bot\"], 'context': ['']}, {'tag': 'recommendation', 'patterns': [\"Recommend me a master's university\", \"I want to do master's abroad\", \"Recommend me some master's universities\"], 'responses': ['Please provide us your details in order to assist you', \"Please mention your field of study, and we will help you find a master's university\"], 'context': ['']}, {'tag': 'field', 'patterns': ['My field of study is computer science', 'My field of study is EEE', 'My field of study is CSE', 'My major is economics', \"Master's field is business administration\"], 'responses': ['Tell me, please, your academic results', 'Tell me your CGPA, IELTS, TOEFL scores to assist you', 'First, tell me about your results'], 'context': ['']}, {'tag': 'ielts_score', 'patterns': ['My IELTS score is 7.5', 'IELTS score is 8', 'My IELTS band is 6.5', 'My IELTS result is 7.0', 'IELTS score is 7.5'], 'responses': ['Great! Now, please provide your TOEFL and GRE scores as well'], 'context': ['']}, {'tag': 'toefl_score', 'patterns': ['My TOEFL score is 100', 'TOEFL score is 110', 'My TOEFL result is 95', 'TOEFL score is 105', 'My TOEFL iBT is 90'], 'responses': ['Perfect! Now, please provide your GRE score'], 'context': ['']}, {'tag': 'gre_score', 'patterns': ['My GRE score is 320', 'GRE score is 330', 'My GRE result is 310', 'GRE score is 325', 'My GRE percentile is 90'], 'responses': ['Excellent! Now, please provide your CGPA'], 'context': ['']}, {'tag': 'cgpa', 'patterns': ['My CGPA is 3.5', 'CGPA is 3', 'My CGPA is 3.8', 'My CGPA is 3.75', 'CGPA is 3.14', 'My result is 3.40', 'CGPA is 2.7', 'CGPA is 3.9', 'CGPA 3.30', 'CGPA is 3.10', 'CGPA is 3.28'], 'responses': ['Tell me, please, your preferred location for pursuing the degree', 'Tell me your preferred location to assist you', 'Fine! First, tell me about your field of interest'], 'context': ['']}, {'tag': 'recommendation_flow', 'patterns': ['My preferred location is {location} and my field of study is {field}', 'I want to study in {location} in {field}', 'I am interested in {field} and prefer to study in {location}'], 'responses': ['Based on your preferences of {location} and academic scores, here are some recommended universities:\\n{universities}'], 'context': ['']}, {'tag': 'no_gre', 'patterns': [\"I haven't taken the GRE\", 'No GRE score', \"Didn't take the GRE\"], 'responses': [\"That's alright! Please provide your TOEFL score\"], 'context': ['']}, {'tag': 'no_toefl', 'patterns': [\"I haven't taken the TOEFL\", 'No TOEFL score', \"Didn't take the TOEFL\"], 'responses': [\"That's alright! Please provide your GRE score\"], 'context': ['']}, {'tag': 'no_gre_toefl', 'patterns': [\"I haven't taken the GRE or TOEFL\", 'No GRE or TOEFL score', \"Didn't take the GRE or TOEFL\"], 'responses': [\"That's alright! Please provide your CGPA score\"], 'context': ['']}], 'recommendations': [{'tag': 'computer_science', 'universities': [{'name': 'University of California, Berkeley', 'location': 'United States', 'ranking': 1, 'gre_min': 320, 'ielts_min': 7.0, 'cgpa_min': 3.5}, {'name': 'Stanford University', 'location': 'United States', 'ranking': 2, 'gre_min': 330, 'ielts_min': 7.5, 'cgpa_min': 3.6}, {'name': 'Massachusetts Institute of Technology (MIT)', 'location': 'United States', 'ranking': 3, 'gre_min': 325, 'ielts_min': 7.0, 'cgpa_min': 3.5}, {'name': 'Harvard University', 'location': 'United States', 'ranking': 4, 'gre_min': 320, 'ielts_min': 7.5, 'cgpa_min': 3.7}, {'name': 'University of California, Los Angeles (UCLA)', 'location': 'United States', 'ranking': 5, 'gre_min': 315, 'ielts_min': 7.0, 'cgpa_min': 3.4}]}, {'tag': 'business_administration', 'universities': [{'name': 'Harvard Business School', 'location': 'United States', 'ranking': 1, 'gre_min': 310, 'ielts_min': 7.0, 'cgpa_min': 3.6}, {'name': 'London Business School', 'location': 'United Kingdom', 'ranking': 2, 'gre_min': 320, 'ielts_min': 7.5, 'cgpa_min': 3.7}, {'name': 'INSEAD', 'location': 'France', 'ranking': 3, 'gre_min': 315, 'ielts_min': 7.0, 'cgpa_min': 3.5}, {'name': 'University of Pennsylvania - The Wharton School', 'location': 'United States', 'ranking': 4, 'gre_min': 310, 'ielts_min': 7.5, 'cgpa_min': 3.6}, {'name': 'Stanford Graduate School of Business', 'location': 'United States', 'ranking': 5, 'gre_min': 320, 'ielts_min': 7.0, 'cgpa_min': 3.5}]}, {'tag': 'engineering', 'universities': [{'name': 'California Institute of Technology (Caltech)', 'location': 'United States', 'ranking': 1, 'gre_min': 320, 'ielts_min': 7.5, 'cgpa_min': 3.5}, {'name': 'ETH Zurich - Swiss Federal Institute of Technology', 'location': 'Switzerland', 'ranking': 2, 'gre_min': 315, 'ielts_min': 7.0, 'cgpa_min': 3.4}, {'name': 'University of Cambridge', 'location': 'United Kingdom', 'ranking': 3, 'gre_min': 325, 'ielts_min': 7.5, 'cgpa_min': 3.6}, {'name': 'Stanford University', 'location': 'United States', 'ranking': 4, 'gre_min': 320, 'ielts_min': 7.0, 'cgpa_min': 3.5}, {'name': 'University of Oxford', 'location': 'United Kingdom', 'ranking': 5, 'gre_min': 315, 'ielts_min': 7.5, 'cgpa_min': 3.4}]}, {'tag': 'psychology', 'universities': [{'name': 'University of Oxford', 'location': 'United Kingdom', 'ranking': 1, 'gre_min': 315, 'ielts_min': 7.5, 'cgpa_min': 3.4}, {'name': 'University of Toronto', 'location': 'Canada', 'ranking': 2, 'gre_min': 310, 'ielts_min': 7.0, 'cgpa_min': 3.6}, {'name': 'University of Melbourne', 'location': 'Australia', 'ranking': 3, 'gre_min': 320, 'ielts_min': 7.5, 'cgpa_min': 3.7}, {'name': 'University of California, Berkeley', 'location': 'United States', 'ranking': 4, 'gre_min': 315, 'ielts_min': 7.0, 'cgpa_min': 3.4}, {'name': 'University of British Columbia', 'location': 'Canada', 'ranking': 5, 'gre_min': 310, 'ielts_min': 7.5, 'cgpa_min': 3.6}]}]}\n"
     ]
    }
   ],
   "source": [
    "with open('intents.json') as file:\n",
    "    data = json.load(file)\n",
    "print(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#for another class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = LancasterStemmer() #stemmer to get stem of a word. ex. 'say' would be stem word of 'saying'.\n",
    "\n",
    "\n",
    "def define_network(X, y):\n",
    "\ttf.compat.v1.reset_default_graph() #Clears the default graph stack and resets the global default graph\n",
    "\t# neural network's layers\n",
    "\tnetwork = tflearn.input_data(shape= [None, len(X[0])]) #input layer\n",
    "\tnetwork = tflearn.fully_connected(network, 8) #1st hidden layer\n",
    "\tnetwork = tflearn.fully_connected(network, 8) #2nd hidden layer\n",
    "\tnetwork = tflearn.fully_connected(network, len(y[0]), activation= 'softmax') #output layer\n",
    "\tnetwork = tflearn.regression(network)\n",
    "\tmodel = tflearn.DNN(network, tensorboard_dir='tflearn_logs') #tensorboard_dir is path to store logs\n",
    "\treturn model\n",
    "\n",
    "\n",
    "# gives stemmed, tokenized words list from sentence pattern without words in ignore_words list\n",
    "def clean_pattern(pattern, ignore_words):\n",
    "    stemmed_pattern = []\n",
    "    wrds = nltk.word_tokenize(pattern)\n",
    "    for w in wrds:\n",
    "        if w not in ignore_words:\n",
    "            stemmed_pattern.append(stemmer.stem(w.lower()))\n",
    "    return stemmed_pattern\n",
    "\n",
    "\n",
    "# generates a numpy array of 0 & 1 from string sentence of user to fed to model\n",
    "def bag_of_words(sentence, stemmed_words, ignore_words):\n",
    "\tbag = []\n",
    "\tstemmed_pattern = clean_pattern(sentence, ignore_words)\n",
    "\tfor w in stemmed_words:\n",
    "\t\tif w in stemmed_pattern:\n",
    "\t\t\tbag.append(1)\n",
    "\t\telse:\n",
    "\t\t\tbag.append(0)\n",
    "\treturn np.array(bag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"'s\", '100', '105', '110', '2.7', '3', '3.10', '3.14', '3.28', '3.30', '3.40', '3.5', '3.75', '3.8', '3.9', '310', '320', '325', '330', '6.5', '7.0', '7.5', '8', '90', '95', 'a', 'abroad', 'admin', 'am', 'and', 'anyon', 'ar', 'band', 'busy', 'bye', 'cgpa', 'comput', 'cse', 'did', 'do', 'econom', 'ee', 'field', 'for', 'goodby', 'gre', 'hav', 'hay', 'hello', 'help', 'hey', 'hi', 'i', 'ibt', 'ielt', 'in', 'interest', 'is', 'lat', 'loc', 'maj', 'mast', 'me', 'my', \"n't\", 'no', 'of', 'or', 'percentil', 'pref', 'prefer', 'recommend', 'result', 'sci', 'scor', 'see', 'som', 'study', 'tak', 'thank', 'that', 'the', 'ther', 'to', 'toefl', 'univers', 'want', 'what', 'who', 'you', '{', '}']\n",
      "['about', 'cgpa', 'field', 'goodbye', 'gre_score', 'greeting', 'ielts_score', 'no_gre', 'no_gre_toefl', 'no_toefl', 'recommendation', 'recommendation_flow', 'thanks', 'toefl_score']\n",
      "[(['hi'], 'greeting'), (['hey'], 'greeting'), (['is', 'anyon', 'ther'], 'greeting'), (['hello'], 'greeting'), (['hay'], 'greeting'), (['bye'], 'goodbye'), (['see', 'you', 'lat'], 'goodbye'), (['goodby'], 'goodbye'), (['thank'], 'thanks'), (['thank', 'you'], 'thanks'), (['that', \"'s\", 'help'], 'thanks'), (['thank', 'for', 'the', 'help'], 'thanks'), (['who', 'ar', 'you'], 'about'), (['what', 'ar', 'you'], 'about'), (['who', 'you', 'ar'], 'about'), (['recommend', 'me', 'a', 'mast', \"'s\", 'univers'], 'recommendation'), (['i', 'want', 'to', 'do', 'mast', \"'s\", 'abroad'], 'recommendation'), (['recommend', 'me', 'som', 'mast', \"'s\", 'univers'], 'recommendation'), (['my', 'field', 'of', 'study', 'is', 'comput', 'sci'], 'field'), (['my', 'field', 'of', 'study', 'is', 'ee'], 'field'), (['my', 'field', 'of', 'study', 'is', 'cse'], 'field'), (['my', 'maj', 'is', 'econom'], 'field'), (['mast', \"'s\", 'field', 'is', 'busy', 'admin'], 'field'), (['my', 'ielt', 'scor', 'is', '7.5'], 'ielts_score'), (['ielt', 'scor', 'is', '8'], 'ielts_score'), (['my', 'ielt', 'band', 'is', '6.5'], 'ielts_score'), (['my', 'ielt', 'result', 'is', '7.0'], 'ielts_score'), (['ielt', 'scor', 'is', '7.5'], 'ielts_score'), (['my', 'toefl', 'scor', 'is', '100'], 'toefl_score'), (['toefl', 'scor', 'is', '110'], 'toefl_score'), (['my', 'toefl', 'result', 'is', '95'], 'toefl_score'), (['toefl', 'scor', 'is', '105'], 'toefl_score'), (['my', 'toefl', 'ibt', 'is', '90'], 'toefl_score'), (['my', 'gre', 'scor', 'is', '320'], 'gre_score'), (['gre', 'scor', 'is', '330'], 'gre_score'), (['my', 'gre', 'result', 'is', '310'], 'gre_score'), (['gre', 'scor', 'is', '325'], 'gre_score'), (['my', 'gre', 'percentil', 'is', '90'], 'gre_score'), (['my', 'cgpa', 'is', '3.5'], 'cgpa'), (['cgpa', 'is', '3'], 'cgpa'), (['my', 'cgpa', 'is', '3.8'], 'cgpa'), (['my', 'cgpa', 'is', '3.75'], 'cgpa'), (['cgpa', 'is', '3.14'], 'cgpa'), (['my', 'result', 'is', '3.40'], 'cgpa'), (['cgpa', 'is', '2.7'], 'cgpa'), (['cgpa', 'is', '3.9'], 'cgpa'), (['cgpa', '3.30'], 'cgpa'), (['cgpa', 'is', '3.10'], 'cgpa'), (['cgpa', 'is', '3.28'], 'cgpa'), (['my', 'prefer', 'loc', 'is', '{', 'loc', '}', 'and', 'my', 'field', 'of', 'study', 'is', '{', 'field', '}'], 'recommendation_flow'), (['i', 'want', 'to', 'study', 'in', '{', 'loc', '}', 'in', '{', 'field', '}'], 'recommendation_flow'), (['i', 'am', 'interest', 'in', '{', 'field', '}', 'and', 'pref', 'to', 'study', 'in', '{', 'loc', '}'], 'recommendation_flow'), (['i', 'hav', \"n't\", 'tak', 'the', 'gre'], 'no_gre'), (['no', 'gre', 'scor'], 'no_gre'), (['did', \"n't\", 'tak', 'the', 'gre'], 'no_gre'), (['i', 'hav', \"n't\", 'tak', 'the', 'toefl'], 'no_toefl'), (['no', 'toefl', 'scor'], 'no_toefl'), (['did', \"n't\", 'tak', 'the', 'toefl'], 'no_toefl'), (['i', 'hav', \"n't\", 'tak', 'the', 'gre', 'or', 'toefl'], 'no_gre_toefl'), (['no', 'gre', 'or', 'toefl', 'scor'], 'no_gre_toefl'), (['did', \"n't\", 'tak', 'the', 'gre', 'or', 'toefl'], 'no_gre_toefl')]\n"
     ]
    }
   ],
   "source": [
    "# Some cleaning of data in intents.json\n",
    "stemmed_words = []\n",
    "tags = []\n",
    "ignore_words = ['!', '?', '.']\n",
    "corpus = []\n",
    "\n",
    "for intent in data['intents']:\n",
    "    for pattern in intent['patterns']:\n",
    "        stemmed_pattern = clean_pattern(pattern, ignore_words)\n",
    "        stemmed_words.extend(stemmed_pattern)\n",
    "        corpus.append((stemmed_pattern, intent['tag']))\n",
    "    if intent['tag'] not in tags:\n",
    "        tags.append(intent['tag'])\n",
    "\n",
    "# remove duplicates and sort\n",
    "stemmed_words = sorted(list(set(stemmed_words)))\n",
    "tags = sorted(list(set(tags)))\n",
    "\n",
    "print(stemmed_words)\n",
    "print(tags)\n",
    "print(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " ...\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]\n",
      " [0 0 0 ... 0 0 0]]\n",
      "[[0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 1 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 1]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Creating numeric features and labels out of cleaned data\n",
    "X = []\n",
    "y = []\n",
    "for item in corpus:\n",
    "    bag = [] #array of 1 and 0. 1 if stemmed word is present in stemmed pattern\n",
    "    stemmed_pattern = item[0]\n",
    "    for w in stemmed_words:\n",
    "        if w in stemmed_pattern:\n",
    "            bag.append(1)\n",
    "        else:\n",
    "            bag.append(0)\n",
    "\n",
    "    tags_row = [] #array of 1 and 0. 1 for current tag and for everything else 0.\n",
    "    current_tag = item[1]\n",
    "    for tag in tags:\n",
    "        if tag == current_tag:\n",
    "            tags_row.append(1)\n",
    "        else:\n",
    "            tags_row.append(0)\n",
    "\n",
    "    #for each item in corpus, X will be array indicating stemmed words and y array indicating tags\n",
    "    X.append(bag)\n",
    "    y.append(tags_row) \n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "print(X)\n",
    "print(y)\n",
    "\n",
    "# saving variables in pickle to be used by main.py\n",
    "with open('saved_variables.pickle', 'wb') as file:\n",
    "    pickle.dump((stemmed_words, tags, ignore_words, X, y), file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Step: 8959  | total loss: \u001b[1m\u001b[32m0.64755\u001b[0m\u001b[0m | time: 0.023s\n",
      "| Adam | epoch: 1120 | loss: 0.64755 - acc: 0.9443 -- iter: 56/61\n",
      "Training Step: 8960  | total loss: \u001b[1m\u001b[32m0.58728\u001b[0m\u001b[0m | time: 0.026s\n",
      "| Adam | epoch: 1120 | loss: 0.58728 - acc: 0.9498 -- iter: 61/61\n",
      "--\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:f:\\NSU\\cse498R\\498.5\\chatbot_model.tflearn is not in all_model_checkpoint_paths. Manually adding it.\n"
     ]
    }
   ],
   "source": [
    "model = define_network(X, y)\n",
    "model.fit(X, y, n_epoch=1120, batch_size=8, show_metric=True) \n",
    "model.save(\"chatbot_model.tflearn\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
